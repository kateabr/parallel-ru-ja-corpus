{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "import glob\n",
    "import json\n",
    "from pyknp import KNP, DrawTree, BList, Juman\n",
    "from dataclasses import dataclass, asdict\n",
    "import pykakasi\n",
    "\n",
    "knp = KNP()\n",
    "transliterator = pykakasi.kakasi()\n",
    "transliterator.setMode(\"H\",\"a\")\n",
    "transliterator.setMode(\"K\",\"a\")\n",
    "transliterator.setMode(\"r\",\"Hepburn\")\n",
    "converter = transliterator.getConverter()\n",
    "jumanpp = Juman()\n",
    "\n",
    "rnnm = RNNMorphPredictor(language=\"ru\")\n",
    "mystem = Mystem(\n",
    "        mystem_bin=None,\n",
    "        grammar_info=True,\n",
    "        disambiguation=True,\n",
    "        entire_input=True,\n",
    "#         glue_grammar_info=True,\n",
    "#         weight=False,\n",
    "         generate_all=True,\n",
    "         no_bastards=False,\n",
    "#         end_of_sentence=False,\n",
    "#         fixlist=None,\n",
    "        use_english_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, set):\n",
    "            print(obj)\n",
    "            return list(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    id: int\n",
    "    sent_ru: str\n",
    "    sent_jp: str\n",
    "\n",
    "@dataclass\n",
    "class Ids:\n",
    "    start: int\n",
    "    end: int\n",
    "\n",
    "@dataclass\n",
    "class Entry:\n",
    "    id: int\n",
    "    title_ru: str\n",
    "    title_jp: str\n",
    "    link_ru: str\n",
    "    link_jp: str\n",
    "    contents: [Sentence]\n",
    "    join_ids_ru: [Ids]\n",
    "    join_ids_jp: [Ids]\n",
    "    erase_ids_ru: [Ids]\n",
    "    erase_ids_jp: [Ids]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(filename: str, encoding: str = 'utf-8'):\n",
    "        with open(filename, 'r', encoding=encoding) as f:\n",
    "            text = f.read()\n",
    "            obj = json.loads(text)\n",
    "            return Entry(**obj)\n",
    "\n",
    "    def to_json(self, filename: str, encoding: str = 'utf-8'):\n",
    "        with open(filename, 'w', encoding=encoding) as f:\n",
    "            json.dump(asdict(self), f, ensure_ascii=False)\n",
    "\n",
    "@dataclass\n",
    "class jaToken:\n",
    "    id: int\n",
    "    word: str\n",
    "    reading: str\n",
    "    lexeme: str\n",
    "    normalized_lexeme: str\n",
    "    pos: str\n",
    "    pos_extended: str\n",
    "    semantic_info: {str: str}\n",
    "\n",
    "@dataclass\n",
    "class ProcessedEntry:\n",
    "    id: int\n",
    "    title_ru: {}\n",
    "    title_jp: {}\n",
    "    link_ru: str\n",
    "    link_jp: str\n",
    "    contents: {}\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(filename: str, encoding: str = 'utf-8'):\n",
    "        with open(filename, 'r', encoding=encoding) as f:\n",
    "            text = f.read()\n",
    "            obj = json.loads(text)\n",
    "            return Entry(**obj)\n",
    "\n",
    "    def to_json(self, filename: str, encoding: str = 'utf-8'):\n",
    "        with open(filename, 'w', encoding=encoding) as f:\n",
    "            json.dump(asdict(self), f, ensure_ascii=False, cls=SetEncoder)\n",
    "\n",
    "def normalize_lexeme(lexeme, sem_str):\n",
    "    res_list = []\n",
    "    res_list.extend(sem_str.split()[0].split(':'))\n",
    "    if len(res_list)>1:\n",
    "        return res_list[1].split('/')[0]\n",
    "    else:\n",
    "        return lexeme\n",
    "\n",
    "def generate_sem_info(sem_str):\n",
    "    sem_list =[]\n",
    "    sem_list.extend(sem_str.split()[1:])\n",
    "    res_dict = {sem_t.split(':')[0] : sem_t.split(':')[1:] for sem_t in sem_list}\n",
    "    return res_dict\n",
    "\n",
    "def generate_pos_extended(posx_str):\n",
    "    if posx_str == '*':\n",
    "        return ''\n",
    "    return posx_str\n",
    "\n",
    "\n",
    "def ru_sem_info_mystem_nested(token):\n",
    "    if token in ['A', 'ADV', 'ADVPRO', 'ANUM', 'APRO', 'COM', 'CONJ', 'INTJ', 'NUM', 'PART', 'PR', 'S', 'SPRO', 'V']:\n",
    "            return 'pos', token.lower()\n",
    "    elif token in ['praes', 'inpraes', 'praet']:\n",
    "        return 'tense', token\n",
    "    elif token in ['nom', 'gen', 'dat', 'acc', 'ins', 'abl', 'part', 'loc', 'voc']:\n",
    "        return 'case', token\n",
    "    elif token in ['sg', 'pl']:\n",
    "        return 'number', token\n",
    "    elif token in ['ger', 'inf', 'partcp', 'indic', 'imper']:\n",
    "        return 'mood', token\n",
    "    elif token in ['brev', 'plen', 'poss']:\n",
    "        return 'variant', token\n",
    "    elif token in ['supr', 'comp']:\n",
    "        return 'degree', token\n",
    "    elif token in ['1p', '2p', '3p']:\n",
    "        return 'person', token\n",
    "    elif token in ['m', 'f', 'n']:\n",
    "        return 'gender', token\n",
    "    elif token in ['ipf', 'pf']:\n",
    "        return 'aspect', token\n",
    "    elif token in ['act', 'pass']:\n",
    "        return 'voice', token\n",
    "    elif token in ['anim', 'inan']:\n",
    "        return 'anim', token\n",
    "    elif token in ['tran', 'intr']:\n",
    "        return 'transit', token\n",
    "    elif token != \"\":\n",
    "        return 'extra', token\n",
    "\n",
    "def ru_generate_sem_info_mystem(info):\n",
    "    if info is None:\n",
    "        return None\n",
    "    (certain_info, uncertain_info) = info.split('=')\n",
    "    res = {}\n",
    "    for token in certain_info.split(','):\n",
    "        key, value = ru_sem_info_mystem_nested(token)\n",
    "        if key == 'extra':\n",
    "            if 'extra' in res.keys():\n",
    "                res['extra'].append(value)\n",
    "            else:\n",
    "                res['extra'] = [value]\n",
    "        else:\n",
    "            res[key] = value\n",
    "    if len(uncertain_info) > 2:\n",
    "        if uncertain_info.startswith('('):\n",
    "            uncertain_info = uncertain_info[1:-1]\n",
    "        res_temp = []\n",
    "        for token_string in uncertain_info.split('|'):\n",
    "            if token_string == '':\n",
    "                continue\n",
    "            tt = {}\n",
    "            for token in token_string.split(','):\n",
    "                key, value = ru_sem_info_mystem_nested(token)\n",
    "                if key == 'extra':\n",
    "                    if 'extra' in tt.keys():\n",
    "                        tt['extra'].append(value)\n",
    "                    else:\n",
    "                        tt['extra'] = [value]\n",
    "                else:\n",
    "                    tt[key] = value\n",
    "            res_temp.append(tt)\n",
    "        if len(res_temp) > 1:\n",
    "            res['uncertain'] = res_temp\n",
    "        else:\n",
    "            return {**res, **res_temp[0]}\n",
    "    return res\n",
    "\n",
    "# https://github.com/olesar/ruUD/blob/master/conversion/RNCtoUD.md\n",
    "rnnmp2mystem = {'fut': 'inpraes', 'pres': 'praes', 'past': 'praet', 'unknown': 'unknown',\n",
    "'masc': 'm', 'neut': 'n', 'fem': 'f',\n",
    "'ind': 'indic', 'imp': 'imper',\n",
    "'2': '2p', '3': '3p', '1': '1p',\n",
    "'plur': 'pl', 'sing': 'sg',\n",
    "'part': 'partcp', 'trans': 'tran',\n",
    "'sup': 'supr', 'cmp': 'comp',\n",
    "'adj': 'a', 'adp': 'pr', 'det': 'apro', 'verb': 'v', 'pron': 'spro', 'noun': 's'}\n",
    "\n",
    "def ru_generate_sem_info_rnnmp(info):\n",
    "    res = {}\n",
    "    for token_pair in info.lower().split('|'):\n",
    "        if len(token_pair.split('=')) == 2:\n",
    "            res[token_pair.split('=')[0]] = token_pair.split('=')[1]\n",
    "    for key in res:\n",
    "        if res[key] in rnnmp2mystem.keys():\n",
    "            res[key] = rnnmp2mystem[res[key]]\n",
    "    return res\n",
    "\n",
    "def remove_extras(mst_info):\n",
    "    mst_info['uncertain'] = [item for item in mst_info['uncertain'] if len(item.items()) > 0]\n",
    "    return mst_info\n",
    "\n",
    "def remove_uncertainty(mystem_info, rnnmp_info):\n",
    "    mystem_info = remove_extras(mystem_info)\n",
    "\n",
    "    length_cmp = [(len(item.keys()), id) for id, item in enumerate(mystem_info['uncertain'])]\n",
    "    if len(set([item[0] for item in length_cmp])) > 1:\n",
    "        min_attrs = min([item[0] for item in length_cmp])\n",
    "        mystem_info['uncertain'] = [item for item in mystem_info['uncertain'] if len(item.keys()) == min_attrs]\n",
    "\n",
    "    sorted_keywise = {k: [dic[k] for dic in mystem_info['uncertain']] for k in mystem_info['uncertain'][0]}\n",
    "\n",
    "    res = dict(mystem_info)\n",
    "\n",
    "    for key in sorted_keywise:\n",
    "        if len(set(sorted_keywise[key])) == 1:\n",
    "            res[key] = sorted_keywise[key][0]\n",
    "        elif key in rnnmp_info.keys():\n",
    "            if rnnmp_info[key] != 'unknown':# and rnnmp_info[key] in sorted_keywise[key]:\n",
    "                res[key] = rnnmp_info[key]\n",
    "    uncertainty_removed = True\n",
    "    for key in sorted_keywise:\n",
    "        if not key in res.keys():\n",
    "            uncertainty_removed = False\n",
    "            break\n",
    "        else:\n",
    "            for item in res['uncertain']:\n",
    "                del(item[key])\n",
    "    if uncertainty_removed:\n",
    "        del(res['uncertain'])\n",
    "\n",
    "    if 'number' in rnnmp_info.keys() and 'number' in res.keys() and rnnmp_info['number'] != res['number']:\n",
    "        res['number'] = rnnmp_info['number']\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gr(token):\n",
    "    if len(token['analysis']) > 0:\n",
    "        return token['analysis'][0]['gr']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_lex(token):\n",
    "    if len(token[0][0]['analysis']) > 0:\n",
    "        return token[0][0]['analysis'][0]['lex']\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def process_rus_string(rus_string):\n",
    "\n",
    "    mystem_analysis = mystem.analyze(rus_string)\n",
    "    forms = rnnm.predict([token['text'] for token in mystem_analysis if 'analysis' in token.keys()])\n",
    "    temp_res = []\n",
    "\n",
    "    for token in zip([(token, ru_generate_sem_info_mystem(extract_gr(token))) for token in mystem_analysis if 'analysis' in token.keys()],\n",
    "                                    [(ru_generate_sem_info_rnnmp(token.tag), token.score) for token in forms]):\n",
    "\n",
    "        if not token[0][1] is None:\n",
    "            if 'uncertain' in token[0][1].keys():\n",
    "                temp_res.append(remove_uncertainty(token[0][1], token[1][0]))\n",
    "            else:\n",
    "                temp_res.append(token[0][1])\n",
    "        else:\n",
    "            temp_res.append({})\n",
    "        temp_res[-1]['text'] = token[0][0]['text']\n",
    "        temp_res[-1]['lexeme'] = extract_lex(token)\n",
    "        temp_res[-1]['reliability_score'] = str(token[1][1])\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for item in temp_res:\n",
    "        res.append({'text': item['text']})\n",
    "        if 'lexeme' in item.keys():\n",
    "            res[-1]['lexeme'] = item['lexeme']\n",
    "            res[-1]['gr'] = {}\n",
    "            keys = set(item.keys())\n",
    "            keys.remove('text')\n",
    "            keys.remove('lexeme')\n",
    "            for key in keys:\n",
    "                res[-1]['gr'][key] = item[key]\n",
    "\n",
    "    final_res = []\n",
    "    idx = 0\n",
    "\n",
    "    for item in mystem_analysis:\n",
    "        if 'analysis' in item.keys():\n",
    "            final_res.append(res[idx])\n",
    "            idx = idx + 1\n",
    "        else:\n",
    "            final_res.append(item)\n",
    "\n",
    "    return final_res\n",
    "\n",
    "def reconstruct_text(processed_line, pr=False):\n",
    "    if not pr:\n",
    "        return \"\".join(item['text'] for item in processed_line)\n",
    "    print(\"\".join(item['text'] for item in processed_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nlp.ist.i.kyoto-u.ac.jp/index.php?plugin=attach&refer=KNP&openfile=knp_feature.pdf\n",
    "jp_conv = {'略称': 'abbr',\n",
    "           '語幹': 'stem',\n",
    "           #content word\n",
    "           '内容語': 'content',\n",
    "           # Parts of speech\n",
    "           #adjective\n",
    "           '形容詞': 'a',\n",
    "           #adnominal adjective\n",
    "           '連体詞': 'ada',\n",
    "           #adjective that ends with 'i'\n",
    "           'イ形容詞イ段': 'iadj',\n",
    "           #adjective that ends with 'na'\n",
    "           'ナ形容詞': 'naadj',\n",
    "           #adverb\n",
    "           '副詞': 'adv',\n",
    "           #judgemental\n",
    "           '判定詞': 'jud',\n",
    "           #auxiliary verb\n",
    "           '助動詞': 'av',\n",
    "           #conjunction\n",
    "           '接続詞': 'conj',\n",
    "           #demonstrative\n",
    "           '指示詞': 'dem',\n",
    "           #nominal demonstrative\n",
    "           '名詞形態指示詞': 'sdem',\n",
    "           #adnominal adjective demonstrative\n",
    "           '連体詞形態指示詞': 'adadem',\n",
    "           #adverbial demonstrative\n",
    "           '副詞形態指示詞': 'advdem',\n",
    "           #interjection\n",
    "           '感動詞': 'interj',\n",
    "           #noun\n",
    "           '名詞': 's',\n",
    "           #common noun\n",
    "           '普通名詞': 'common',\n",
    "           #adverbial noun\n",
    "           '副詞的名詞': 'advs',\n",
    "           #expletive noun\n",
    "           '形式名詞': 'expletive',\n",
    "           #proper noun\n",
    "           '固有名詞': 'props',\n",
    "           '組織名': 'organisation',\n",
    "           #toponym\n",
    "           '地名': 'geo',\n",
    "           #name and name retrieved through Wikipedia\n",
    "           '人名': 'human_name',\n",
    "           'Wikipedia人名': 'human_name',\n",
    "           #can form a verb by adding \"suru\" to a noun, nominal verb\n",
    "           'サ変名詞': 'sv',\n",
    "           #verbial noun\n",
    "           'サ変動詞': 'vs',\n",
    "           #auxiliary noun\n",
    "           '準内容語': 'as',\n",
    "           #numeral\n",
    "           '数詞': 'num',\n",
    "           #temporal noun\n",
    "           '時相名詞': 'temporal',\n",
    "           #'weak' temporal noun\n",
    "           '弱時相名詞': 'temporal',\n",
    "           #verb\n",
    "           '動詞': 'v',\n",
    "#            #consonantal verb\n",
    "#            '子音動詞ラ行': 'consonantal-ra',\\\n",
    "#            '子音動詞ワ行': 'consonantal-wa',\\\n",
    "#            '子音動詞マ行': 'consonantal-ma',\\\n",
    "#            '母音動詞': 'vowel-stem',\\\n",
    "           #conditional judgemental\n",
    "           'デアル列基本条件形': 'cond',\n",
    "           #past judgemental\n",
    "           'デアル列タ形': 'praet',\n",
    "           #continuous judgemental\n",
    "           'ダ列タ系連用テ形': 'cont',\n",
    "           #imperfective form\n",
    "           '未然形': 'ipf',\n",
    "           #plain form, also applicable to adjectives\n",
    "           '基本形': 'plain',\n",
    "           #irregular verb\n",
    "           'カ変動詞': 'irregular',\n",
    "           #conjunctive form (when occurring within verbial composites) that acts like a verb(?)\n",
    "           '基本連用形': 'conjunctive-v',\n",
    "            #conjunctive form that acts like a noun(?)\n",
    "           '連用形名詞化': 'conjunctive-s',\n",
    "           #\"ta\"-form; past form\n",
    "           'タ形': 'praet',\n",
    "           #continuous tense\n",
    "           'タ系連用テ形': 'cont',\n",
    "           #particle\n",
    "           '助詞': 'part',\n",
    "           #case-marking particle\n",
    "           '格助詞': 'case-marking',\n",
    "           #adverbial particle\n",
    "           '副助詞': 'adverbial',\n",
    "           #conjunctive particle\n",
    "           '接続助詞': 'conjunctive',\n",
    "           #sentence-ending particle\n",
    "           '終助詞': 'sentence-ending',\n",
    "           #prefix\n",
    "           '接頭辞': 'pref',\n",
    "           #nominal prefix\n",
    "           '名詞接頭辞': 'nominal',\n",
    "           #verbal prefix\n",
    "           '動詞接頭辞': 'verbal',\n",
    "           #prefix for adjectives ending with 'i'\n",
    "           'イ形容詞接頭辞': 'iadj',\n",
    "           #prefix for adjectives ending with 'na'\n",
    "           'ナ形容詞接頭辞': 'naadj',\n",
    "           #suffix\n",
    "           '接尾辞': 'suff',\n",
    "           #nominal suffix\n",
    "           '名詞性名詞接尾辞': 'nominal',\n",
    "           #predicative nominal suffix\n",
    "           '名詞性述語接尾辞': 'predicative',\n",
    "           #suffix somehow related to counting\n",
    "           '名詞性名詞助数辞': 'counting',\n",
    "           #special nominal suffix\n",
    "           '名詞性特殊接尾辞': 'special',\n",
    "           #predicative adjective suffix\n",
    "           '形容詞性述語接尾辞': 'predicative',\n",
    "           #suffix that turns a noun into an adjective ('-like')\n",
    "           '形容詞性名詞接尾辞': 'adjectivising',\n",
    "           #verbial suffix\n",
    "           '動詞性接尾辞': 'verbial',\n",
    "           #special symbol\n",
    "           '特殊': 'special',\n",
    "           '句点': 'period',\n",
    "           '読点': 'comma',\n",
    "           #opening parenthesis\n",
    "           '括弧始': 'parenthop',\n",
    "           #ending parenthesis\n",
    "           '括弧終': 'parenthed',\n",
    "           #sign/mark\n",
    "           '記号': 'sign',\n",
    "           '空白': 'whitespace',\n",
    "           #unidentified symbols\n",
    "           '未定義語': 'unspecified',\n",
    "           '未知語': 'unknown_characters',\n",
    "           #katakana characters\n",
    "           'カタカナ': 'katakana',\n",
    "           #alphanumeric characters\n",
    "           'アルファベット': 'alpha',\n",
    "           '数字': 'numeric',\n",
    "           #other characters\n",
    "           'その他': 'other',\n",
    "           #suspected representation (cases such as when 2 is written instead of 二 [two])\n",
    "           '疑似代表表記': 'suspected_representation',\n",
    "           #category\n",
    "           'カテゴリ': 'category',\n",
    "           '人工物-その他': 'artificial-other',\n",
    "           '抽象物': 'abstract',\n",
    "           '組織・団体': 'organisation',\n",
    "           '場所-その他': 'place-other',\n",
    "           '場所-施設': 'place-establishment',\n",
    "           '人': 'human',\n",
    "           '数量': 'quantity',\n",
    "           '時間': 'time',\n",
    "           '場所-機能': 'place-facility',\n",
    "           '自然物': 'natural',\n",
    "           '場所-自然': 'place-natural',\n",
    "           #domain\n",
    "           'ドメイン': 'domain',\n",
    "           '政治': 'politics',\n",
    "           '家庭・暮らし': 'household',\n",
    "           '代表表記': 'writrepr',\n",
    "           '料理・食事': 'cooking-meals',\n",
    "           '文化・芸術': 'culture-art',\n",
    "           'メディア': 'media',\n",
    "           'ビジネス': 'business',\n",
    "           '交通': 'traffic',\n",
    "           #family name\n",
    "           'Wikipedia姓': 'famn',\n",
    "           '姓': 'famn',\n",
    "           #first name\n",
    "           'Wikipedia名': 'persn',\n",
    "           '名': 'persn',\n",
    "           #name type\n",
    "           '日本': 'japanese',\n",
    "           '外国': 'foreign',\n",
    "           #special reading\n",
    "           '漢字読み': 'reading_type',\n",
    "           '音': 'on',\n",
    "           '訓': 'kun',\n",
    "           #address ender\n",
    "           '住所末尾': 'address_ender',\n",
    "           #toponym ender\n",
    "           '地名末尾': 'toponym_ender',\n",
    "           #reading unknown\n",
    "           '読み不明': 'unknown_reading',\n",
    "           '自動獲得': 'acquired_automatically',\n",
    "           #reflexivity\n",
    "           '自他動詞': 'transitivity',\n",
    "           '自': 'tran',\n",
    "           '他': 'intran',\n",
    "           '同形': 'same_form',\n",
    "           #word looked up at Wikipedia\n",
    "           'Wikipediaリダイレクト': 'wiki_redirected',\n",
    "           #word derived from verb\n",
    "           '動詞派生': 'derivative',\n",
    "           #potential form of a verb\n",
    "           '可能動詞': 'potential'\n",
    "          }\n",
    "\n",
    "def conv2eng(string):\n",
    "    if string in jp_conv.keys():\n",
    "        return jp_conv[string]\n",
    "    return string\n",
    "\n",
    "def is_suspected(token):\n",
    "    if token.imis != 'NIL':\n",
    "        for item in [it.split(':') for it in token.imis.split()]:\n",
    "            if conv2eng(item[0]) == 'suspected_representation':\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def process_ja_string(ja_string):\n",
    "    parsed_str = knp.parse(ja_string)\n",
    "    res = []\n",
    "    for item in parsed_str.mrph_list():\n",
    "        res.append({'text': item.midasi})\n",
    "        if conv2eng(item.hinsi) == \"special\" or item.midasi == \"～\":\n",
    "            continue\n",
    "        res[-1]['gr'] = {}\n",
    "        res[-1]['gr']['extra'] = set([])\n",
    "        res[-1]['lexeme'] = item.genkei\n",
    "        res[-1]['reading'] = converter.do(item.yomi)\n",
    "        res[-1]['gr']['unsorted'] = []\n",
    "        # TODO\n",
    "        #res[-1]['translation']\n",
    "        res[-1]['gr']['pos'] = conv2eng(item.hinsi)\n",
    "        if item.bunrui != '*':\n",
    "            res[-1]['gr']['extra'].add(conv2eng(item.bunrui))\n",
    "        if item.katuyou1 != '*':\n",
    "            if conv2eng(item.katuyou1) != item.katuyou1:\n",
    "                res[-1]['gr']['extra'].add(conv2eng(item.katuyou1))\n",
    "            else:\n",
    "                res[-1]['gr']['unsorted'].append(item.katuyou1)\n",
    "        if item.katuyou2 != '*':\n",
    "            if conv2eng(item.katuyou2) in ['ipf', 'pf']:\n",
    "                res[-1]['gr']['aspect'] = conv2eng(item.katuyou2)\n",
    "            elif conv2eng(item.katuyou2) in ['inpraes', 'praet']:\n",
    "                res[-1]['gr']['tense'] = conv2eng(item.katuyou2)\n",
    "            elif conv2eng(item.katuyou2) != item.katuyou2:\n",
    "                res[-1]['gr']['extra'].add(conv2eng(item.katuyou2))\n",
    "            else:\n",
    "                res[-1]['gr']['unsorted'].append(item.katuyou2)\n",
    "        if item.imis != 'NIL':\n",
    "            split_sem_info = [it.split(':') for it in item.imis.split()]\n",
    "            for sem_it in split_sem_info:\n",
    "                if conv2eng(sem_it[0]) == 'writrepr':\n",
    "                    res[-1]['normal_form'] = sem_it[1].split('/')[0]\n",
    "                elif conv2eng(sem_it[0]) == 'category':\n",
    "                    res[-1]['gr']['category'] = [conv2eng(cat_it) for cat_it in sem_it[1].split(';')]\n",
    "                elif conv2eng(sem_it[0]) == 'domain':\n",
    "                    res[-1]['gr']['domain'] = [conv2eng(dom_it) for dom_it in sem_it[1].split(';')]\n",
    "                elif conv2eng(sem_it[0]) == 'geo':\n",
    "                    for id, geo_it in enumerate(sem_it):\n",
    "                        if conv2eng(geo_it) == 'abbr':\n",
    "                            res[-1]['gr']['extra'].add('abbr')\n",
    "                            res[-1]['normal_form'] = sem_it[id + 1]\n",
    "                            break\n",
    "                elif conv2eng(sem_it[0]) == 'reading_type':\n",
    "                    res[-1]['gr']['reading_type'] = conv2eng(sem_it[1])\n",
    "                elif conv2eng(sem_it[0]) == 'conjunctive-s':\n",
    "                    res[-1]['normal_form'] = sem_it[1].split('/')[0]\n",
    "                elif conv2eng(sem_it[0]) == 'transitivity':\n",
    "                    if conv2eng(sem_it[1]) == 'intran':\n",
    "                        #sem_it[1] can also be equal to '同形',\n",
    "                        #apparently meaning that transitive and intransitive forms are the same\n",
    "                        #since knp provides no information on whether the verb in question\n",
    "                        #is transitive or intransitive, the 'transitivity' field is left out\n",
    "                        res[-1]['gr']['transit'] = \"tran\"\n",
    "                        res[-1]['normal_form'] = sem_it[2].split('/')[0]\n",
    "                    elif conv2eng(sem_it[1]) == \"tran\":\n",
    "                        res[-1]['gr']['transit'] = \"intran\"\n",
    "                elif conv2eng(sem_it[0]) == 'derivative':\n",
    "                    res[-1]['gr']['extra'].add(conv2eng(sem_it[0]))\n",
    "                    res[-1]['normal_form'] = sem_it[1].split('/')[0]\n",
    "                elif conv2eng(sem_it[0]) == 'potential':\n",
    "                    res[-1]['gr']['extra'].add(conv2eng(sem_it[0]))\n",
    "                    res[-1]['normal_form'] = sem_it[1].split('/')[0]\n",
    "                elif conv2eng(sem_it[0]) == 'human_name':\n",
    "                    if not sem_it[0].startswith('Wikipedia'):\n",
    "                        if conv2eng(sem_it[1]) == \"japanese\":\n",
    "                            res[-1]['gr']['extra'].add(conv2eng(sem_it[2]))\n",
    "                            res[-1]['gr']['reliability_score'] = sem_it[4]\n",
    "                        else:\n",
    "                            res[-1]['gr']['extra'].add(conv2eng(sem_it[0]))\n",
    "                            res[-1]['gr']['extra'].add(conv2eng(sem_it[1]))\n",
    "                            res[-1]['gr']['reliability_score'] = 0\n",
    "                    else:\n",
    "                        res[-1]['gr']['extra'].add(conv2eng(sem_it[0]))\n",
    "                elif conv2eng(sem_it[0]) == 'wiki_redirected':\n",
    "                    res[-1]['gr']['extra'].add('acquired_automatically')\n",
    "                    res[-1]['normal_form'] = sem_it[1]\n",
    "                elif conv2eng(sem_it[0]) != sem_it[0]:\n",
    "                    res[-1]['gr']['extra'].add(conv2eng(sem_it[0]))\n",
    "                else:\n",
    "                    res[-1]['gr']['unsorted'].append([it for it in sem_it])\n",
    "\n",
    "            if \"unknown_characters\" in res[-1]['gr']['extra']:\n",
    "                del(res[-1]['normal_form'])\n",
    "                del(res[-1]['lexeme'])\n",
    "                unk_type = conv2eng([it.split(':')[1] for it in item.imis.split(' ')\\\n",
    "                             if conv2eng(it.split(':')[0]) == \"unknown_characters\"][0])\n",
    "                res[-1]['gr']['extra'].add(unk_type)\n",
    "                if unk_type != \"katakana\" or res[-1]['reading'] == res[-1]['text']:\n",
    "                    del(res[-1]['reading'])\n",
    "\n",
    "        if len(res[-1]['gr']['unsorted']) == 0:\n",
    "            del(res[-1]['gr']['unsorted'])\n",
    "        if len(res[-1]['gr']['extra']) == 0:\n",
    "            del(res[-1]['gr']['extra'])\n",
    "\n",
    "    #print(res)\n",
    "    return ja_replace_sets_with_lists(res)\n",
    "\n",
    "#sets are used because some pieces of information that KNP yields have identical mappings\n",
    "def ja_replace_sets_with_lists(token_set):\n",
    "    res = token_set\n",
    "    for token_idx in range(0, len(res)):\n",
    "        if 'gr' in res[token_idx].keys() and 'extra'\\\n",
    "        in res[token_idx]['gr'].keys():\n",
    "            res[token_idx]['gr']['extra'] = list(res[token_idx]['gr']['extra'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = glob.glob('*.txt')\n",
    "fnames = list(enumerate([fname for fname in txts]))\n",
    "\n",
    "entries = {first : Entry.from_json(second) for (first, second) in fnames}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "\n",
    "for _, entry in list(entries.items()):\n",
    "    title_ja = process_ja_string(entry.title_jp)\n",
    "    title_ru = process_rus_string(entry.title_ru)\n",
    "\n",
    "    body = {}\n",
    "\n",
    "    for line in entry.contents:\n",
    "        body[line[0]] = (process_rus_string(line[1][0]), process_ja_string(line[1][1]))\n",
    "\n",
    "    processed.append(ProcessedEntry(id = entry.id,\n",
    "                                   title_ru = title_ru,\n",
    "                                   contents = body,\n",
    "                                   title_jp = title_ja,\n",
    "                                   link_jp = entry.link_jp,\n",
    "                                   link_ru = entry.link_ru))\n",
    "\n",
    "    processed[-1].to_json(\"./processed_biling/processed_entry_{}.json\".format(processed[-1].id))\n",
    "    print(entry.id, \": Success\")\n",
    "\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for text in processed:\n",
    "    print(text)\n",
    "    text.to_json(\"./processed_ru/processed_entry_{}.json\".format(text.id))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Что проверять\n",
    "\n",
    "#extra:famn на русском\n",
    "#низкие reliability_score на русском\n",
    "#пустая лексема\n",
    "\n",
    "#unknown_reading\n",
    "#acquired_automatically\n",
    "#suspected_representation\n",
    "#reliability_score\n",
    "#human_name + foreign\n",
    "#verbs without transitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В городе Хакусан нашлись кровати русских военнопленных, арестованных во время русско-японской войны\n",
      "\n",
      "ロシア捕虜のベッドか　日露戦争時、金沢で収容　白山市で見つかる\n"
     ]
    }
   ],
   "source": [
    "print(reconstruct_text(processed[-1].title_ru))\n",
    "print(reconstruct_text(processed[-1].title_jp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}